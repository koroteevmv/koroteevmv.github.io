---
title: "Лучшие научные публикации о машинном обучении 2020 года"
classes: None
categories:
  - scipop
tags:
  - translation
  - article
  - machine-learning
toc: true
excerpt: "Такая статья должна была выйти давно. Даже при всей поверхностности, приведенная экспертом подборка дает общее представление о тенденциях в изучении машинного обучения. Внутри много ссылок на источники."
---

[Оригинал](https://analyticsindiamag.com/top-machine-learning-research-papers-released-in-2020/)

Уже в середине последнего месяца года, сайт arxiv.org, популярный репозиторий исследовательских работ поуже засвидетельствовал около 600 загрузок. Это должно дать представление о темпах, с которыми продвигаются исследования в области машинного обучения; однако отследить всю эту исследовательскую работу практически невозможно. Каждый год исследования, которые получают максимум шума, обычно исходят от таких компаний, как Google и Facebook; от ведущих университетов, таких как MIT; из исследовательских лабораторий; и, что наиболее важно, по результатам конференций, таких как NeurIPS или ACL. 



*   [CVPR](https://analyticsindiamag.com/everything-so-far-in-cvpr-2020-conference/): 1470 допущенных работ по компьютерному зрению принято из 6656 заявок.
*   [ICLR](https://analyticsindiamag.com/iclr-2020-conference-papers/): 687 из 2594 статей попали на ICLR 2020 - показатель принятия 26,5%.
*   [ICML](https://analyticsindiamag.com/papers-icml-2020-research-conference/): из 4990 заявок было принято 1088 статей.

В этой статье мы составили список интересных исследовательских работ по машинному обучению, которые наделали шума в этом году. 


## Обработка естественного языка


### GPT-3

Это основополагающая [статья,](https://arxiv.org/pdf/2005.14165.pdf) в которой представлена ​​самая популярная модель машинного обучения года - GPT-3. В статье под названием «Трансформеры мало обучаются» команда OpenAI использовала ту же модель и архитектуру, что и GPT-2, которая включает модифицированную инициализацию, предварительную нормализацию и обратимую токенизацию, а также чередование плотных и локально полосчатых шаблонов разреженного внимания в слои трансформатора. В то время как модель GPT-3 показывала многообещающие результаты в настройках нулевого и одноразового выстрелов, в настройке нескольких выстрелов она иногда превосходила самые современные модели. 


### АЛЬБЕРТ: облегченный BERT

Обычно увеличение размера модели при предварительном обучении представлений на естественном языке часто приводит к повышению производительности последующих задач, но время обучения увеличивается. Для решения этих проблем авторы в своей [работе](https://analyticsindiamag.com/iclr-2020-conference-papers/) представили два метода уменьшения параметров, чтобы снизить потребление памяти и увеличить скорость обучения BERT. Авторы также использовали самоконтролируемую функцию потерь, которая фокусируется на моделировании согласованности между предложениями и помогает целевым задачам, зависящим от нескольких предложений. Согласно результатам, эта модель показала новые рекордные результаты в тестах GLUE, RACE и отряда, имея меньшее количество параметров по сравнению с BERT-large. 

Прочитайте статью [здесь](https://arxiv.org/pdf/1909.11942v6.pdf).


### За пределами точности: поведенческое тестирование моделей НЛП с помощью CheckList

Microsoft Research вместе с Вашингтонским и Калифорнийским университетами в этой статье представила методологию, не зависящую от модели и не зависящую от задачи, для тестирования моделей НЛП, известную как CheckList (контрольный список). Он также получил награду за лучшую работу на конференции ACL в этом году. Он включал матрицу общих лингвистических возможностей и типов тестов, которые облегчают всестороннее формирование идей тестирования, а также программный инструмент для быстрого создания большого и разнообразного количества тестовых примеров. 

Прочитайте статью [здесь](https://arxiv.org/abs/2005.04118).


### Linformer

Linformer - это архитектура трансформеров, предназначенная для устранения узкого места в них, связанного с самовниманием. Он снижает сложность механизма самовнимания до O(n) операций как в пространственной, так и во временной сложности. Это новый механизм самовнимания, который позволяет исследователям вычислять контекстное отображение за линейное время и память вне зависимости от длины последовательности. 

Подробнее о бумаге [здесь](https://arxiv.org/abs/2006.04768).


### Языковые модели Plug and Play 

Языковые модели Plug and Play ([PPLM](https://analyticsindiamag.com/what-are-plug-and-play-language-models-pplm-nlp/)) представляют собой комбинацию предварительно обученных языковых моделей с одним или несколькими простыми классификаторами атрибутов. Это, в свою очередь, помогает в создании текста без какого-либо дополнительного обучения. По словам авторов, образцы моделей продемонстрировали контроль над настроением генерируемого текста, а обширные автоматизированные и аннотированные оценки показали согласованность атрибутов и беглость. 

Прочитайте статью  [здесь](https://arxiv.org/pdf/1912.02164v4.pdf).


### Reformer 

В этом исследователи Google [статье](https://arxiv.org/pdf/2001.04451v2.pdf%5C) представили Reformer. Эта работа продемонстрировала, что архитектура Transformer может эффективно выполняться на длинных последовательностях и с небольшой памятью. Авторы считают, что способность обрабатывать длинные последовательности открывает возможность использования Reformer для решения многих генеративных задач. Помимо создания очень длинного связного текста, Reformer может использовать возможности моделей-трансформеров в других областях, таких как прогнозирование временных рядов, создание музыки, изображений и видео. 

Прочитайте статью  [здесь](https://arxiv.org/pdf/2001.04451v2.pdf%5C).


### Performers

Чтобы преодолеть ограничения разреженных трансформеров, Google в другой статье представил Performer, который использует эффективную (линейную) структуру обобщенного внимания и может напрямую повлиять на исследования в области анализа биологической последовательности и т. д. Авторы заявили, что современная биоинформатика может получить огромную пользу от более быстрых и точных языковых моделей для разработки новых вакцин на основе наночастиц. 

Прочитайте статью  [здесь](https://analyticsindiamag.com/transformers-attention-google-introduces-performers/).


## Компьютерное зрение


### Изображение стоит 16x16 слов

![alt_text](/assets/images/2021-05-09/image1.png "image_tooltip")


Ирония заключается в том, что одна из популярных языковых моделей, трансформеры, была создана для решения задач компьютерного зрения. В этой [статье](https://analyticsindiamag.com/computer-vision-text-caption-google/4) авторы утверждали, что визуальные трансформеры могут идти в ногу с современными моделями в тестах распознавания изображений, достигая точности 88,36% в ImageNet и 94,55% в CIFAR-100. Для этого трансформер компьютерного зрения получает входные данные в виде одномерной последовательности эмбеддингов токенов. Затем изображение преобразуется в последовательность плоских 2D-участков. Трансформеры в этой работе используют постоянную ширину на всех своих слоях.

Прочитайте статью  [здесь](https://openreview.net/pdf?id=YicbFdNTTy).


### Обучение без учителя предположительно симметричных деформируемых трехмерных объектов

![alt_text](/assets/images/2021-05-09/image3.png "image_tooltip")


Победитель премии CVPR за лучшую статью, в этой работе авторы предложили метод изучения категорий трехмерных деформируемых объектов из необработанных одноэкранных изображений без внешнего контроля. В этом методе используется автоэнкодер, который учитывает каждое входное изображение по глубине, альбедо, точке обзора и освещенности. Авторы продемонстрировали, что рассуждения об освещении могут использоваться для использования симметрии основного объекта, даже если внешний вид не является симметричным из-за затенения.

Прочитайте статью  [здесь](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Unsupervised_Learning_of_Probably_Symmetric_Deformable_3D_Objects_From_Images_CVPR_2020_paper.pdf).


### Генеративное предварительное обучение из пикселей

В этой [статье](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) исследователи OpenAI изучили, могут ли модели обучаться полезным представлениям изображений. Для этого исследователи обучили последовательный трансформер авторегрессионному предсказанию пикселей, не используя знания входной 2D-структуры. Несмотря на обучение в ImageNet с низким разрешением без классов, исследователи обнаружили, что масштабная модель GPT-2 изучает четкие представления изображений, измеренные с помощью линейного зондирования, точной настройки и классификации с низким объемом данных. На CIFAR-10 он достиг точности 96,3%, превзойдя контролируемую Wide ResNet, и точность 99,0% с полной тонкой настройкой и соответствием верхним контролируемым предварительно обученным моделям. Еще более крупная модель, обученная на смеси ImageNet и веб-изображений, может конкурировать с самоконтролируемыми тестами на ImageNet, достигая 72,0% точности (наилучший результат на сегодняшний день).

Прочитайте статью  [здесь](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf).


## Обучение с подкреплением


### Глубокое обучение с подкреплением и его нейробиологические последствия

В этой статье авторы представили высокоуровневое введение в [глубокий RL](https://analyticsindiamag.com/deep-reinforcement-learning-neuroscience-applications/), обсудили некоторые из его первоначальных приложений в нейробиологии, а также рассмотрели его более широкое значение для исследований мозга и поведения и завершили список. возможностей для следующего этапа исследования. Хотя DeepRL кажется многообещающим, авторы писали, что работа над ним все еще продолжается, и его применение в нейробиологии следует рассматривать как прекрасную возможность. Например, глубокий RL обеспечивает основанную на агентах структуру для изучения того, как вознаграждение формирует представление, и как представление, в свою очередь, влияет на обучение и принятие решений - две проблемы, которые вместе охватывают большую часть того, что является наиболее важным для нейробиологии. 

Прочитайте статью  [здесь](https://arxiv.org/pdf/2007.03750.pdf).


### Обучение с подкреплением на основе дофамина

Причины того, почему люди делают определенные вещи, часто связаны с [дофамином](https://analyticsindiamag.com/deepmind-neuroscience-reinforcement-learning-dopamine-temporal-difference/), гормоном, который действует как система вознаграждения (например: лайки на вашей странице в Instagram). Итак, оглядываясь назад, DeepMind с помощью лабораторий Гарварда проанализировал дофаминовые клетки у мышей и зафиксировал, как мыши получали вознаграждение, когда они изучали задачу. Затем они проверили эти записи на соответствие активности дофаминовых нейронов с помощью стандартных алгоритмов временной разницы. В этой статье был предложен отчет об обучении с подкреплением на основе дофамина, вдохновленный недавними исследованиями искусственного интеллекта по распределенному обучению с подкреплением. Авторы предположили, что мозг представляет возможные будущие награды не как единое среднее значение, а как распределение вероятностей, эффективно представляя несколько будущих результатов одновременно и параллельно. 

Прочитайте статью  [здесь](https://www.nature.com/articles/s41586-019-1924-6).


### Лотерейные билеты в обучении с подкреплением и НЛП

В этой статье авторы объединили обработку естественного языка (НЛП) и обучение с подкреплением (RL). Они исследовали как повторяющиеся модели LSTM, так и крупномасштабные модели, основанные на трансформерах для НЛП и пространственные задачи дискретного действия для RL. Результаты показали, что гипотеза лотерейного билета не ограничивается контролируемым обучением естественных изображений, а скорее представляет собой более широкое явление в глубоких нейронных сетях.

Прочитайте статью  [здесь](https://openreview.net/pdf?id=S1xnXRVFwH).


### Что можно получить за счет внутреннего вознаграждения

![alt_text](/assets/images/2021-05-09/image2.png "image_tooltip")


В этой статье авторы исследовали, может ли [функция вознаграждения](https://analyticsindiamag.com/reward-function-reinforcement-learning/) сама по себе быть хорошим локусом усвоенных знаний. Они предложили масштабируемую структуру для изучения полезных внутренних функций вознаграждения на протяжении нескольких жизненных циклов опыта и показали, что возможно изучить и зафиксировать знания о долгосрочном исследовании и эксплуатации в качестве функции вознаграждения. 

Прочитайте статью  [здесь](https://arxiv.org/pdf/1912.05500).


## Разное


### AutoML-Zero

Развитие AutoML в основном сосредоточено на архитектуре нейронных сетей, где в качестве строительных блоков используются сложные слои, спроектированные экспертами, или аналогичные ограничительные пространства поиска. В этой [статье](https://analyticsindiamag.com/automl-drawbacks-automl-zero-google-brain-new-approach/) авторы показали, что AutoML может пойти дальше с AutoML Zero, который автоматически обнаруживает полные алгоритмы машинного обучения, просто используя базовые математические операции в качестве строительных блоков. Исследователи продемонстрировали это, представив новую структуру, которая значительно снизила человеческую предвзятость за счет универсального пространства поиска.

Прочитайте статью  [здесь](https://arxiv.org/pdf/2003.03384v1.pdf).


### Переосмысление пакетной нормализации для метаобучения.

Пакетная нормализация - важный компонент конвейеров метаобучения. Однако есть несколько проблем. Итак, в этой статье авторы оценили ряд подходов к пакетной нормализации для сценариев метаобучения и разработали новый подход - TaskNorm. Эксперименты показали, что выбор пакетной нормализации оказывает значительное влияние как на точности классификации и время обучения и для подходов [мета-обучения](https://analyticsindiamag.com/how-to-make-meta-learning-more-effective/), основанных на градиенте, так и на безградиентных подходов. Было обнаружено, что TaskNorm постоянно улучшает производительность.

Прочитайте статью [здесь](https://arxiv.org/pdf/2003.03284).


### Метаобучение без запоминания

Алгоритмы метаобучения требуют, чтобы задачи были взаимоисключающими, чтобы ни одна модель не могла решить все задачи одновременно. В этой статье авторы разработали цель мета-регуляризации, используя теорию информации, которая успешно использует данные из не исключающих друг друга задач для эффективной адаптации к новым задачам.

Прочитайте статью  [здесь](https://arxiv.org/pdf/1912.03820v3.pdf).


### Понимание эффективности MAML

Метаобучение, не зависящее от модели (MAML) состоит из циклов оптимизации, в которых внутренний цикл может эффективно изучать новые задачи. В этой статье авторы продемонстрировали, что повторное использование функций является доминирующим фактором и привело к алгоритму ANIL (Almost No Inner Loop) - упрощению MAML, в котором внутренний цикл удаляется для всех, кроме (зависящей от задачи) головы лежащей в основе нейронной сети. сеть. 

Прочитайте статью  [здесь](https://openreview.net/pdf?id=rkgMkCEtPB).


### Ваш классификатор втайне является энергетической моделью

В этой статье предлагается попытаться переосмыслить стандартный дискриминантый классификатор как модель, основанную на энергии. В этом случае, как пишут авторы, стандартные вероятности классов могут быть легко вычислены. Они [продемонстрировали](https://analyticsindiamag.com/classifier-architecture-energy-based-models-deep-learning-google/), что обучение совместного распределения на основе энергии улучшает калибровку, надежность, обнаружение раздачи-распределения, а также позволяет предлагаемой модели генерировать выборки, конкурирующие по качеству с последними подходами к GAN. Эта работа улучшает недавно предложенные методы для расширения обучения энергетических моделяй. Такой подход также первым достиг производительности, соперничающей с современными достижениями как в генеративном, так и в дискриминантом обучении в рамках одной гибридной модели.

Прочитайте статью [здесь](https://arxiv.org/pdf/1912.03263).


### Обратный инжиниринг глубоких сетей ReLU 

В этой статье исследуется общепринятое мнение о том, что нейронные сети не могут быть восстановлены по его выходным данным, поскольку они зависят от его параметров очень нелинейным образом. Авторы утверждали, что, наблюдая только его выходные данные, можно определить архитектуру, веса и смещения неизвестной глубокой сети ReLU. Разделив набор границ области на компоненты, связанные с конкретными нейронами, исследователи показали, что можно восстановить веса нейронов и их расположение в сети.

Прочитайте статью  [здесь](https://arxiv.org/pdf/1910.00744.pdf).

(Примечание: список составлен без определенного порядка и представляет собой компиляцию, основанную на репутации издателей, восприятии этой исследовательской работы на популярных форумах и отзывах экспертов в социальных сетях. Если вы считаете, что мы пропустили какую-либо исключительную исследовательскую работу, пожалуйста, прокомментируйте ниже)
